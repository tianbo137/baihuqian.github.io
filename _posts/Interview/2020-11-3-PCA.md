---
layout: post
title: "PCA"
date: '2020-11-3'
tags:
 - DataScience
---

Generally, in real world dataset, we have the following problem:

     High Dimensions =  Lot of Features   
     
The primary problem associated with high-dimensionality in the machine learning field is model overfitting, which reduces the ability to generalize beyond the examples in the training set. Richard Bellman described this phenomenon in 1961 as the Curse of Dimensionality: 
> "Many algorithms that work fine in low dimensions become intractable when the input is high-dimensional".

PCA is a powerful unsupervised learning technique for taking a orthogonal projection of a high-dimensional data into a (possibly lower dimensional) subspace so  that the variance of the projected data is maximized, so that we are not losing too much information. It is one of the simplest and most robust ways of doing **dimensionality reduction**.  

Useful for:
* Visualization        
* Further processing by machine learning algorithms    
* More efficient use of resources (e.g., time, memory, communication)    
* Statistical: fewer dimensions and a better generalization    
* Noise removal (improving data quality)    
   
### 1. Mathematics of Principal Components

Given a number of $$N$$ vectors $$\{ \vec{x}_i \in \mathbb{R}^n \}$$ (the sample data), and we want to summarize them by projecting down into a $$k$$-dimensional subspace. The summary of the given data will be the projection of the original vectors on to $$k$$ directions, the principal components, which span the subspace. 

There are several equivalent ways of deriving the principal components mathematically. The simplest one is by finding the projections which maximize the variance:

$$
\begin{align}
\end{align}
$$

The first principal component is the direction in space along which projections have the largest variance. The second principal component is the direction which maximizes variance among all directions orthogonal to the first. The $$k$$-th component is the variance-maximizing direction orthogonal to the previous $$k−1$$ components. There are $$k$$-principal components in all. 


Another approach would be, rather than maximizing variance, to look for the projection with the smallest average (mean-squared) distance between the original vectors and their projections on to the principal components; this turns out to beequivalent to maximizing the variance.

Throughout, we will assume that the data have been “centered”, so that every variable has mean 0:

$$
\begin{align}
	\vec{\mu}_x = \frac{1}{N} \sum^N_{i=1} \vec{x}_i
\end{align}
$$


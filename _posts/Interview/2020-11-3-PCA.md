---
layout: post
title: "PCA"
date: '2020-11-3'
tags:
 - DataScience
---


    High Dimensions =  Lot of Features    

PCA is a powerful unsupervised learning technique for taking high-dimensional data and using the dependencies between the variables to extract hidden (potentially lower dimensional) structure from high dimensional datasets, without losing too much information.  

Useful for:
* Visualization        
* Further processing by machine learning algorithms    
* More efficient use of resources (e.g., time, memory, communication)    
* Statistical: fewer dimensions and a better generalization    
* Noise removal (improving data quality)    
   
### 1. Mathematics of Principal Components

Given a number of $$N$$ vectors $$\{ \vec{x}_i \in \mathbb{R}^n \}$$ (the sample data), and we want to summarize them by projecting down into a $$k$$-dimensional subspace. The summary of the given data will be the projection of the original vectors on to $$k$$ directions, the principal components, which span the subspace. 

There are several equivalent ways of deriving the principal components mathematically. The simplest one is by finding the projections which maximize the variance:

$$
\begin{align}
\end{align}
$$

The first principal component is the direction in space along which projections have the largest variance. The second principal component is the direction which maximizes variance among all directions orthogonal to the first. The $$k$$-th component is the variance-maximizing direction orthogonal to the previous $$k−1$$ components. There are $$k$$-principal components in all. 


Another approach would be, rather than maximizing variance, to look for the projection with the smallest average (mean-squared) distance between the original vectors and their projections on to the principal components; this turns out to beequivalent to maximizing the variance.

Throughout, we will assume that the data have been “centered”, so that every variable has mean 0:

$$
\begin{align}
	\vec{\mu}_x = \frac{1}{N} \sum^N_{i=1} \vec{x}_i
\end{align}
$$


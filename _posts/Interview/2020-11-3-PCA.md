---
layout: post
title: "PCA"
date: '2020-11-3'
tags:
 - DataScience
---

Generally, in real world dataset, we have the following problem:

     High Dimensions =  Lot of Features   
     
The primary problem associated with high-dimensionality in the machine learning field is model overfitting, which reduces the ability to generalize beyond the examples in the training set. Richard Bellman described this phenomenon in 1961 as the Curse of Dimensionality: 
> "Many algorithms that work fine in low dimensions become intractable when the input is high-dimensional".

PCA is a powerful unsupervised learning technique for taking a orthogonal projection of a high-dimensional data into a (possibly lower dimensional) subspace so  that the variance of the projected data is maximized, so that we are not losing too much information. It is one of the simplest and most robust ways of doing **dimensionality reduction**.  

PCA is useful for the following purposes:
* Visualization        
* Further processing by machine learning algorithms    
* More efficient use of resources (e.g., time, memory, communication)    
* Statistical: fewer dimensions and a better generalization    
* Noise removal (improving data quality)    
   
### 1. Mathematics of Principal Components

Given a number of $$N$$ vectors $$\{ \vec{x}_i \in \mathbb{R}^n \}$$ (In machine laerning, $$n$$ denotes the number of features), and we want to summarize them by projecting down into a $$k$$-dimensional subspace. The summary of the given data will be the projection of the original vectors on to $$k$$ directions, the principal components, which span the subspace. 

Before we explain the mathematical idea behind PCA algorithm, we need to understand inner product and change of bases in $$\mathbb{R}^n$$.

#### 1.1 Inner Product and Matrix Transformation

Let $$\vec{x}$$ and $$\vec{y}$$ be two vectors in $$\mathbb{R}^n$$ such that 

$$
\begin{align}

\vec{x} = \sum a_{i} \vec{e}_i, \ \ \   \vec{y} = \sum b_{i} \vec{e}_i,  

\end{align}
$$
 
here, $$\vec{e}_i, \ i = 1, 2, \cdots, n$$ are *any* orthonormal basis of $$\mathbb{R}^n$$. For example, in the three-dimensional space $$\mathbb{R}^n$$, we could choosen the following standard basis:

$$
\begin{align}
\vec{e}_1 = (1,0,0),\quad \vec{e}_2 = (0,1,0),\quad \vec{e}_3=(0,0,1).
\end{align}
$$

We define the inner product of $$\vec{x}$$ and $$\vec{y}$$ as

$$\langle \vec{x},\  \vec{y} \rangle = ( a_1,a_2,\cdots,a_n ) \cdot \left( \begin{array}{c} b_1 \\ b_2 \\ \vdots \\ b_n \end{array} \right) = a_1b_1+a_2b_2+\cdots+a_nb_n.$$

which can be written as 

$$\langle \vec{x}, \ \vec{y} \rangle = |\vec{x}||\vec{y}|cos(\theta)$$

where $$\theta$$ is the angle between these two vectors $$\vec{x}$$ and $$\vec{y}$$. Geometrically speaking，the inner product of $$\vec{x}$$ and $$\vec{y}$$ computes the length of the projection of $$\vec{x}$$ onto $$\vec{y}$$.

Given any vector $$\vec{x} = \sum a_{i} \vec{e}_i$$ with respect to the orthonormal bases $$\{\vec{e}_1, \cdots, \vec{e}_n \}$$, its representation under a different orthonormal basis $$\{\vec{f}_1, \cdots, \vec{f}_n \}$$ is given by 

$$ \vec{x} & = \sum \langle \vec{x}, \ \vec{f}_i \rangle \vec{f}_i = (\vec{f}_1, \vec{f}_2, \cdots, \vec{f}_n) \cdot \left( \begin{array}{c}\langle \vec{x}, \ \vec{f}_1 \rangle\\ \langle \vec{x}, \ \vec{f}_2 \rangle \\ \vdots \\ \langle \vec{x}, \ \vec{f}_n \rangle \end{array} \right)\\
& = $$

#### 1.2 Coviarance 

There are several equivalent ways of deriving the principal components mathematically. The simplest one is by finding the projections which maximize the variance:

$$
\begin{align}
\end{align}
$$

The first principal component is the direction in space along which projections have the largest variance. The second principal component is the direction which maximizes variance among all directions orthogonal to the first. The $$k$$-th component is the variance-maximizing direction orthogonal to the previous $$k−1$$ components. There are $$k$$-principal components in all. 


Another approach would be, rather than maximizing variance, to look for the projection with the smallest average (mean-squared) distance between the original vectors and their projections on to the principal components; this turns out to beequivalent to maximizing the variance.

Throughout, we will assume that the data have been “centered”, so that every variable has mean 0:

$$
\begin{align}
	\vec{\mu}_x = \frac{1}{N} \sum^N_{i=1} \vec{x}_i
\end{align}
$$

